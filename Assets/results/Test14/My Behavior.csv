Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
50000,1.4202366,46.42884615384615,12.611499,1.3178056662075788,1.3178056662075788,41.971527,0.023848355,0.0002845932,0.1948644,0.004743734,1.0
100000,1.4216018,53.380065717415114,9.858378,1.6842451853681706,1.6842451853681706,21.440723,0.023626039,0.00025688243,0.18562749,0.004282811,1.0
150000,1.4222894,53.90502183406114,7.5006666,1.7021289300580211,1.7021289300580211,12.111236,0.02113171,0.00022608919,0.17536303,0.0037706154,1.0
200000,1.420821,58.23741007194245,6.187752,1.925570360788492,1.925570360788492,7.2682395,0.022641167,0.00019528494,0.16509496,0.0032582388,1.0
250000,1.420059,57.07342657342657,4.9738646,1.8624564751687789,1.8624564751687789,4.9267826,0.022374893,0.00016446775,0.15482256,0.0027456456,1.0
300000,1.4202623,62.05689001264223,4.207586,2.1201012808212587,2.1201012808212587,3.094688,0.022332441,0.00013364143,0.14454713,0.002232901,1.0
350000,1.4202132,60.877300613496935,3.5613291,2.058036949444402,2.058036949444402,2.6420245,0.021101804,0.00010284643,0.13428211,0.0017206777,1.0
400000,1.4197767,61.810776942355886,3.0696557,2.1107278802388882,2.1107278802388882,1.9572885,0.024182977,7.5121134e-05,0.12504037,0.0012595133,1.0
450000,1.4196348,60.799752781211374,2.570866,2.0512964442188357,2.0512964442188357,1.737463,0.022255102,4.7381014e-05,0.11579366,0.00079810276,1.0
500000,1.4183431,68.15955983493811,2.3085616,2.4358817229339897,2.4358817229339897,1.1381867,0.024276093,1.6573536e-05,0.105524495,0.00028567156,1.0
