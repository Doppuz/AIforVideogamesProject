Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
50000,1.4212978,53.09836065573771,3.7231517,1.6764224387493385,1.6764224387493385,5.977907,0.023360338,0.00028461328,0.1948711,0.004744068,1.0
100000,1.4286984,57.69203747072599,2.843057,1.9047425247955658,1.9047425247955658,2.28258,0.026780466,0.0002569001,0.18563335,0.0042831046,1.0
150000,1.4304341,55.0381593714927,2.3715456,1.7703143756932815,1.7703143756932815,1.4966689,0.022615805,0.00022610214,0.17536739,0.0037708315,1.0
200000,1.4315518,52.51931330472103,1.8893836,1.6494099890789249,1.6494099890789249,0.97625005,0.022065565,0.00019532484,0.16510826,0.0032589033,1.0
250000,1.4271516,56.23147092360319,1.6297873,1.8312430071857935,1.8312430071857935,0.6957556,0.022891607,0.00016452259,0.15484086,0.0027465578,1.0
300000,1.4226162,58.71186440677966,1.504878,1.952724102514708,1.952724102514708,0.57160753,0.021292673,0.00013374511,0.14458169,0.002234626,1.0
350000,1.4189032,60.67661691542288,1.4407289,2.0575777897553413,2.0575777897553413,0.49699172,0.02130911,0.000102988524,0.13432948,0.0017230411,1.0
