Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
100000,1.4098828,97.58583690987125,-1.7531581,-0.06652333117349976,-0.06652333117349976,1.603313,0.022362744,0.0002845228,0.19484097,0.004742563,1.0
200000,1.3936316,139.65345080763583,-0.2784073,2.1235687445613958,2.1235687445613958,1.1580775,0.024541298,0.00025514787,0.18504928,0.00425396,1.0
300000,1.3871439,214.75262054507337,0.7978706,6.082458778088834,6.082458778088834,1.1461976,0.023003649,0.00022429394,0.17476463,0.0037407551,1.0
400000,1.384573,405.0352422907489,1.9804679,16.10701919961394,16.10701919961394,0.74945444,0.024534762,0.00019490335,0.16496776,0.0032518916,1.0
500000,1.3734645,648.476821192053,3.1850722,29.86523451394593,29.86523451394593,0.42601958,0.024629343,0.00016553557,0.1551785,0.0027634075,1.0
600000,1.3640091,851.5384615384615,4.033552,41.33205492476113,41.33205492476113,0.2209965,0.02457432,0.00013463231,0.1448774,0.002249383,1.0
700000,1.3640511,965.59,4.5025716,47.875004136562346,47.875004136562346,0.0985382,0.026404263,0.000105264284,0.13508806,0.0017608947,1.0
800000,1.3641679,908.561403508772,4.692864,44.72500388664112,44.72500388664112,0.1186499,0.023180313,7.5899006e-05,0.12529965,0.0012724521,1.0
900000,1.362148,948.6538461538462,4.786746,46.85144636722711,46.85144636722711,0.104507014,0.023776427,4.5002096e-05,0.11500068,0.0007585335,1.0
1000000,1.3603257,988.8247422680413,4.8274603,49.335055788767704,49.335055788767704,0.044402584,0.022873025,1.4093651e-05,0.10469786,0.00024442276,1.0
