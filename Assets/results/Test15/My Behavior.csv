Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
50000,1.4175162,58.29032258064516,3.3598602,-2.0125596876920127,-2.0125596876920127,21.920277,0.021600414,0.0002845823,0.19486073,0.004743551,1.0
100000,1.416869,54.24414715719063,1.93264,-2.247045605477681,-2.247045605477681,11.342346,0.024845403,0.00025681933,0.18560645,0.0042817607,1.0
150000,1.4194978,59.82656826568266,1.5178845,-1.9436038165074874,-1.9436038165074874,7.1084433,0.021584298,0.0002259976,0.17533253,0.003769093,1.0
200000,1.4140551,61.96745932415519,0.9760866,-1.8247498553991317,-1.8247498553991317,5.4683466,0.023482427,0.00019519805,0.165066,0.0032567934,1.0
250000,1.4159846,59.61853188929001,0.39255884,-1.9559034858841493,-1.9559034858841493,3.8492408,0.024461951,0.00016437858,0.15479285,0.0027441627,1.0
300000,1.4155619,62.549936788874845,0.10520318,-1.798610978656345,-1.798610978656345,3.1740754,0.019952392,0.00013359368,0.1445312,0.0022321069,1.0
350000,1.4142586,61.51596424010217,-0.20287123,-1.8532565775806724,-1.8532565775806724,2.5927062,0.022782592,0.00010280864,0.13426952,0.0017200488,1.0
400000,1.4131544,63.949935815147626,-0.3387982,-1.7354754341292198,-1.7354754341292198,2.3557985,0.024270725,7.5050484e-05,0.12501681,0.0012583383,1.0
450000,1.412337,56.82981220657277,-0.6053488,-2.100820514549264,-2.100820514549264,2.1021025,0.02314275,4.7300844e-05,0.11576692,0.0007967693,1.0
500000,1.4117894,63.0126582278481,-0.5654811,-1.7790504926367652,-1.7790504926367652,1.9898441,0.022722814,1.6454262e-05,0.105484724,0.0002836876,1.0
