Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
100000,1.4117974,53.844430217669654,-3.3170698,-2.307778408829595,-2.307778408829595,1.539481,0.023852734,0.00029227693,0.19742566,0.00487154,1.0
200000,1.3692695,340.32,-0.8250265,12.554668005837334,12.554668005837334,0.73342115,0.021410905,0.00027759437,0.19253144,0.004627319,1.0
300000,1.335457,711.9148936170212,1.2272402,33.60425833607396,33.60425833607396,0.3778997,0.023311183,0.0002620868,0.18736224,0.0043693767,1.0
400000,1.3130904,776.4596774193549,3.2316906,36.99960007590632,36.99960007590632,0.28741926,0.02159468,0.00024739697,0.18246563,0.0041250354,1.0
500000,1.3032838,785.3307086614174,4.239423,37.679765245271106,37.679765245271106,0.3065227,0.022743259,0.00023271069,0.17757025,0.0038807536,1.0
600000,1.292536,842.5169491525423,4.575714,40.829415345392306,40.829415345392306,0.21386847,0.024542639,0.00021720007,0.17240001,0.0036227598,1.0
700000,1.2825881,834.8389830508474,4.6965647,40.50805441201744,40.50805441201744,0.2069584,0.02343364,0.0002024923,0.16749741,0.003378121,1.0
800000,1.2727182,765.375,4.7480235,36.78008139319718,36.78008139319718,0.26342577,0.02471739,0.00018780805,0.16260268,0.0031338725,1.0
900000,1.264488,829.008064516129,4.779495,40.36008418567719,40.36008418567719,0.2158602,0.022585075,0.00017235047,0.15745012,0.0028767618,1.0
1000000,1.2552507,918.9345794392524,4.785132,45.28878898932555,45.28878898932555,0.10484156,0.02317301,0.00015767684,0.15255894,0.0026326901,1.0
