Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
100000,1.4206889,106.95704057279237,0.61038977,0.40811488793402695,0.40811488793402695,2.381297,0.022528265,0.0002845813,0.19486043,0.004743534,1.0
200000,1.4010265,339.95620437956205,1.6682899,12.80876047767862,12.80876047767862,0.8341379,0.023048084,0.00025517918,0.18505974,0.00425448,1.0
300000,1.3868777,513.8930481283422,2.908251,22.287970063520625,22.287970063520625,0.6497133,0.0235775,0.00022429737,0.1747658,0.0037408122,1.0
400000,1.3753015,640.0700636942676,3.6094549,29.383760654242934,29.383760654242934,0.4665887,0.024266453,0.00019500479,0.16500157,0.0032535782,1.0
500000,1.3583199,732.3284671532847,4.0491695,34.56533157912484,34.56533157912484,0.35903016,0.023922585,0.00016571123,0.15523706,0.0027663286,1.0
600000,1.3492712,778.7952755905512,4.41352,37.27992458043136,37.27992458043136,0.2514047,0.023284936,0.00013480321,0.14493439,0.0022522255,1.0
700000,1.3392757,857.7304347826087,4.5412426,41.57522104097449,41.57522104097449,0.2103971,0.023456747,0.00010544054,0.13514683,0.0017638265,1.0
800000,1.3354548,837.0909090909091,4.6466055,40.61074737674934,40.61074737674934,0.1998221,0.022132281,7.6103664e-05,0.12536785,0.0012758562,1.0
900000,1.3302,905.8666666666667,4.688495,44.4730808024223,44.4730808024223,0.14593454,0.024352882,4.5175948e-05,0.11505864,0.00076142506,1.0
1000000,1.3287739,893.1666666666666,4.75882,43.87609078158503,43.87609078158503,0.13984561,0.023065487,1.4258587e-05,0.10475282,0.0002471662,1.0
