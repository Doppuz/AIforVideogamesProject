Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
100000,1.4180349,101.72780373831776,0.44231653,0.1571848865981414,0.1571848865981414,2.3823593,0.024296843,0.0002845703,0.19485678,0.004743352,1.0
200000,1.3946729,467.019801980198,1.6473693,19.47599200801094,19.47599200801094,0.7167867,0.023472998,0.00025521984,0.18507329,0.004255156,1.0
300000,1.3714906,628.8490566037735,2.8777325,28.37987684453808,28.37987684453808,0.49937338,0.025013903,0.00022433649,0.17477882,0.0037414625,1.0
400000,1.3540037,766.4715447154472,3.7572017,36.15650732536626,36.15650732536626,0.34668872,0.022277465,0.0001950133,0.16500443,0.0032537205,1.0
500000,1.3388736,826.982905982906,4.2641864,39.802140274618424,39.802140274618424,0.26745456,0.022095747,0.00016569658,0.15523219,0.0027660853,1.0
600000,1.331078,896.4561403508771,4.5586433,43.85395120528706,43.85395120528706,0.1697993,0.023810202,0.00013481916,0.1449397,0.002252491,1.0
700000,1.3283625,939.2162162162163,4.740614,46.43273129896684,46.43273129896684,0.09544748,0.022095095,0.00010392553,0.13464181,0.0017386267,1.0
800000,1.3246758,897.6160714285714,4.7890515,44.126552529039635,44.126552529039635,0.14019515,0.02549991,7.4581156e-05,0.124860354,0.0012505316,1.0
900000,1.3225628,929.9428571428572,4.808688,45.9312539765468,45.9312539765468,0.10260657,0.022570843,4.5203393e-05,0.11506778,0.0007618817,1.0
1000000,1.3220307,935.3679245283018,4.840014,46.28177970814928,46.28177970814928,0.095826045,0.02435304,1.4311865e-05,0.104770586,0.0002480524,1.0
