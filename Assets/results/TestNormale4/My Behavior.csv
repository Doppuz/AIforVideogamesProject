Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
100000,1.4214525,72.56853582554517,0.943468,-1.3361758314931875,-1.3361758314931875,3.4047673,0.023005523,0.00028458438,0.19486144,0.0047435868,1.0
200000,1.4097211,136.13669064748203,0.69321525,1.8867630378805476,1.8867630378805476,1.5114753,0.025765445,0.0002552818,0.18509392,0.0042561875,1.0
300000,1.3896878,286.7692307692308,1.5241677,9.861391664962092,9.861391664962092,0.9805342,0.023883045,0.00022438324,0.1747944,0.00374224,1.0
400000,1.3736992,374.21771217712177,2.5390308,14.828968295312016,14.828968295312016,0.77205867,0.023389403,0.00019501626,0.1650054,0.0032537694,1.0
500000,1.3638406,397.59071729957805,3.0398185,16.00886236967417,16.00886236967417,0.8164531,0.021759182,0.00016563447,0.15521146,0.0027650525,1.0
600000,1.3487792,479.8341463414634,3.4286985,20.61756295227423,20.61756295227423,0.6791404,0.02284876,0.00013474235,0.1449141,0.0022512136,1.0
700000,1.3422941,585.6432748538011,3.8167586,26.497078474502118,26.497078474502118,0.4902668,0.022267349,0.000105368985,0.13512295,0.001762636,1.0
800000,1.3389927,545.2044198895028,3.989347,24.32486414382471,24.32486414382471,0.559699,0.022152385,7.598243e-05,0.12532744,0.0012738396,1.0
900000,1.3361804,595.3882352941176,4.039133,27.086473094715792,27.086473094715792,0.46697825,0.02300104,4.5119938e-05,0.115039945,0.00076049345,1.0
1000000,1.3340647,582.3571428571429,4.083786,26.282145285890216,26.282145285890216,0.5348364,0.023202261,1.4222647e-05,0.10474084,0.0002465684,1.0
