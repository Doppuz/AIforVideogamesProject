Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
100000,1.4118618,101.28055878928987,0.67957866,0.10518075229125196,0.10518075229125196,2.0861542,0.024757408,0.00028454393,0.194848,0.0047429134,1.0
200000,1.382312,382.1625,1.8325274,14.760418197512626,14.760418197512626,0.8016008,0.024378147,0.00025521877,0.18507293,0.0042551383,1.0
300000,1.3610446,639.2941176470588,3.089796,29.110133400150374,29.110133400150374,0.49562788,0.024167955,0.00022435367,0.17478454,0.0037417482,1.0
400000,1.3476166,722.8958333333334,3.9198542,34.02048917777009,34.02048917777009,0.39693156,0.021606691,0.00019504198,0.16501398,0.003254197,1.0
500000,1.3348827,766.7727272727273,4.397562,36.552275982770055,36.552275982770055,0.32657403,0.023509799,0.00016567713,0.15522571,0.0027657622,1.0
600000,1.3283157,856.2991452991453,4.553493,41.78590110224536,41.78590110224536,0.19052199,0.023314465,0.00013483685,0.14494559,0.0022527848,1.0
700000,1.3212616,888.9818181818182,4.619106,43.33670104752987,43.33670104752987,0.16581044,0.021559786,0.00010400447,0.13466811,0.0017399394,1.0
800000,1.3149918,925.6213592233009,4.735036,45.492311658767555,45.492311658767555,0.13521716,0.023966052,7.467908e-05,0.124893,0.0012521608,1.0
900000,1.3118585,945.6792452830189,4.83179,46.6669851878904,46.6669851878904,0.1092121,0.023430482,4.5331915e-05,0.11511061,0.0007640193,1.0
1000000,1.3088475,946.8490566037735,4.8575215,46.868400283579554,46.868400283579554,0.07857926,0.024586204,1.4473718e-05,0.10482454,0.00025074458,1.0
