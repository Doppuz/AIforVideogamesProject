Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
100000,1.417082,81.83669885864794,-0.32182106,-0.8859962686516927,-0.8859962686516927,3.1305058,0.023267241,0.00028461212,0.19487071,0.0047440478,1.0
200000,1.3911129,274.5792682926829,0.74806017,9.144665702087122,9.144665702087122,1.0415636,0.023377262,0.00025532563,0.18510851,0.004256916,1.0
300000,1.3678429,517.7888888888889,2.0767734,22.33491833782729,22.33491833782729,0.58983326,0.022674367,0.00022446162,0.17482053,0.0037435438,1.0
400000,1.353116,761.75,3.2274208,36.2335798827401,36.2335798827401,0.3569481,0.021444108,0.00019512315,0.16504104,0.0032555473,1.0
500000,1.3356935,832.6428571428571,4.016955,40.104021404470714,40.104021404470714,0.27231377,0.022230031,0.00016579035,0.15526344,0.0027676448,1.0
600000,1.327572,844.2131147540983,4.3660555,40.894675739475936,40.894675739475936,0.23210935,0.025200367,0.00013489474,0.1449649,0.0022537478,1.0
700000,1.3221338,849.2894736842105,4.560324,41.229828196659426,41.229828196659426,0.18716767,0.02193883,0.000103998915,0.1346663,0.0017398472,1.0
800000,1.3208815,879.2260869565217,4.680696,42.82565593304842,42.82565593304842,0.21496095,0.022991072,7.4671116e-05,0.12489036,0.0012520282,1.0
900000,1.319267,933.8703703703703,4.765702,46.08889290138527,46.08889290138527,0.095619805,0.024634523,4.5359317e-05,0.11511974,0.0007644751,1.0
1000000,1.3180487,904.045871559633,4.789786,44.372022221941464,44.372022221941464,0.13595632,0.023682648,1.4487337e-05,0.10482909,0.0002509711,1.0
