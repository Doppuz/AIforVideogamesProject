Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
100000,1.4151553,22.17134765164031,-0.8180815,-3.8914326205691387,-3.8914326205691387,4.836865,0.023324594,0.0002845918,0.19486392,0.0047437097,1.0
200000,1.3871658,46.3114837398374,-2.100871,-2.684425746037708,-2.684425746037708,1.2664001,0.023499332,0.00025528605,0.18509537,0.004256258,1.0
300000,1.359608,87.69027384324835,-1.1662502,-0.6154860797302131,-0.6154860797302131,1.2420785,0.022896323,0.00022436774,0.17478923,0.0037419826,1.0
400000,1.338534,151.73037037037037,-0.12661082,2.586519028345744,2.586519028345744,1.2221622,0.026208613,0.00019499983,0.16499992,0.0032534963,1.0
500000,1.3181182,206.6209476309227,0.83589536,5.331048154474197,5.331048154474197,1.0808957,0.024319857,0.0001655974,0.15519911,0.0027644357,1.0
600000,1.3017366,332.77186311787074,1.7569392,11.638594471456434,11.638594471456434,0.96524954,0.0245498,0.00013453394,0.14484462,0.0022477466,1.0
700000,1.2926008,485.2795698924731,2.5523758,19.263980497596084,19.263980497596084,0.8426557,0.025386352,0.000105041676,0.13501388,0.0017571917,1.0
800000,1.2834629,554.2448979591836,3.1061301,22.71224719164323,22.71224719164323,0.755443,0.024363028,7.564862e-05,0.12521617,0.0012682873,1.0
900000,1.2800169,809.6944444444445,3.4950979,35.484725671785846,35.484725671785846,0.6297212,0.021568755,4.4692617e-05,0.11489751,0.00075338566,1.0
1000000,1.2775817,877.9605263157895,3.8914998,38.898030055196664,38.898030055196664,0.48551717,0.02305337,1.5322199e-05,0.10510737,0.00026485766,1.0
