Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
100000,1.4184495,29.561277705345503,-1.4133389,-3.5219361029050495,-3.5219361029050495,3.038456,0.02362686,0.00028454926,0.19484976,0.0047430024,1.0
200000,1.4008493,53.159849300322925,-2.0307996,-2.3248248869331536,-2.3248248869331536,1.1887207,0.02491528,0.00025525142,0.18508379,0.0042556813,1.0
300000,1.3567758,94.37512846865364,-1.2227179,-0.2694555738868165,-0.2694555738868165,1.2087642,0.025131315,0.00022438662,0.17479554,0.0037422967,1.0
400000,1.3119218,185.73651452282158,0.24432899,4.370643829409017,4.370643829409017,1.128049,0.0231695,0.00019503818,0.16501272,0.0032541347,1.0
500000,1.2811999,294.6386292834891,1.5011269,10.062306464275467,10.062306464275467,1.0039651,0.024043735,0.00016567775,0.15522589,0.002765772,1.0
600000,1.2614372,448.6543778801843,2.5052912,18.270508767273018,18.270508767273018,0.8177811,0.020625588,0.00013471456,0.14490484,0.002250751,1.0
700000,1.2474618,531.7921348314607,3.2653375,22.866294362571804,22.866294362571804,0.6799822,0.023059417,0.00010530594,0.13510196,0.0017615876,1.0
800000,1.235459,582.9644970414201,3.7622688,26.03077167731065,26.03077167731065,0.6449711,0.023908542,7.591182e-05,0.12530392,0.0012726651,1.0
900000,1.2295754,584.433734939759,4.026984,25.955725348139385,25.955725348139385,0.6042186,0.023143763,4.4955417e-05,0.11498511,0.000757757,1.0
1000000,1.2266363,695.2328767123288,4.1943545,32.14829059986219,32.14829059986219,0.5141369,0.022696465,1.3966928e-05,0.10465561,0.000242315,1.0
