Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
100000,1.4276065,33.259355316783996,1.712597,-3.334074870011609,-3.334074870011609,8.895985,0.022666069,0.000284582,0.19486065,0.0047435467,1.0
200000,1.4074734,66.52387740555952,-0.30113515,-1.645907311083159,-1.645907311083159,2.1645799,0.023442153,0.00025529184,0.18509728,0.0042563537,1.0
300000,1.369089,133.09735744089014,-0.056778,1.7321284036145588,1.7321284036145588,1.3481191,0.02119897,0.00022444737,0.17481579,0.003743307,1.0
400000,1.3283432,251.9971181556196,1.1148409,7.7308366910181405,7.7308366910181405,1.0214714,0.02623748,0.00019503238,0.16501077,0.0032540371,1.0
500000,1.3028797,341.2,2.1321898,12.601072803565435,12.601072803565435,1.0258709,0.024633437,0.00016553576,0.15517858,0.0027634106,1.0
600000,1.281706,478.8781725888325,2.8873963,20.046194870459853,20.046194870459853,0.76044047,0.022886898,0.00013454733,0.14484909,0.0022479696,1.0
700000,1.2641927,550.0411764705882,3.429974,23.77941405773163,23.77941405773163,0.69496846,0.024180457,0.00010513978,0.13504657,0.0017588238,1.0
800000,1.256249,623.3473053892216,3.84463,28.37485292428982,28.37485292428982,0.5358711,0.023027925,7.5668664e-05,0.12522286,0.0012686208,1.0
900000,1.2498163,669.8653846153846,4.0633225,30.791669498651455,30.791669498651455,0.52447903,0.02521128,4.4746463e-05,0.11491547,0.00075428135,1.0
1000000,1.2468429,704.5655172413793,4.2742434,32.596554705192304,32.596554705192304,0.4821378,0.023438295,1.5372698e-05,0.105124205,0.0002656976,1.0
