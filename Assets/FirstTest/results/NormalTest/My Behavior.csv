Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
100000,1.4198703,30.681119836121542,-1.1195091,-3.4641489892973936,-3.4641489892973936,4.4434934,0.026413348,0.0002845939,0.19486463,0.004743745,1.0
200000,1.3989625,65.96689655172413,-1.5864809,-1.6811078946834914,-1.6811078946834914,1.2467527,0.024229942,0.00025531594,0.18510532,0.0042567546,1.0
300000,1.352899,167.20458891013385,-0.36476502,3.418165034591354,3.418165034591354,1.0201483,0.022477292,0.00022449301,0.17483099,0.0037440662,1.0
400000,1.3216797,328.54578754578756,1.0462806,11.741759542143825,11.741759542143825,0.8096587,0.023908043,0.00019512215,0.1650407,0.0032555312,1.0
500000,1.2948217,530.7885714285715,2.3644438,22.89571648189,22.89571648189,0.6184426,0.024186734,0.00016569883,0.15523292,0.0027661226,1.0
600000,1.2770047,682.9791666666666,3.472095,31.517960625635066,31.517960625635066,0.41945228,0.023831079,0.00013473936,0.14491309,0.0022511638,1.0
700000,1.2645857,757.5846153846154,4.050524,35.859851716142714,35.859851716142714,0.38386372,0.023988687,0.000105305604,0.13510185,0.001761582,1.0
800000,1.255352,761.1007751937984,4.416663,36.30426681134128,36.30426681134128,0.27754614,0.024066338,7.5896365e-05,0.12529875,0.0012724081,1.0
900000,1.2489268,784.4263565891473,4.594704,37.66628241723822,37.66628241723822,0.25836694,0.024125686,4.499289e-05,0.1149976,0.0007583802,1.0
1000000,1.2464249,863.965811965812,4.6745405,41.95342250359364,41.95342250359364,0.20816693,0.020903211,1.4027612e-05,0.104675844,0.00024332435,1.0
