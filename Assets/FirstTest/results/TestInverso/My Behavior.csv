Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
100000,1.4107714,48.80592991913747,-2.5389407,-2.552723757143031,-2.552723757143031,1.2945371,0.023293432,0.0002845954,0.19486512,0.004743769,1.0
200000,1.3613651,181.46296296296296,-1.0198466,4.167762462608134,4.167762462608134,0.94906867,0.022483569,0.00025530666,0.18510222,0.004256601,1.0
300000,1.3157117,570.577380952381,0.91008574,25.06190714240074,25.06190714240074,0.5709181,0.023720518,0.00022438457,0.17479484,0.0037422627,1.0
400000,1.2860807,686.536231884058,2.6171837,31.559060865554258,31.559060865554258,0.42325136,0.023517093,0.00019503165,0.16501051,0.0032540257,1.0
500000,1.2651174,760.8188405797101,3.707836,35.968481494032815,35.968481494032815,0.3556593,0.021634866,0.00016570781,0.15523592,0.0027662725,1.0
600000,1.258491,897.1509433962265,4.411836,43.85943780080328,43.85943780080328,0.17565069,0.025111308,0.00013478383,0.14492792,0.002251903,1.0
700000,1.2522628,913.5233644859813,4.7568364,44.9238356875482,44.9238356875482,0.14275311,0.023794524,0.000105406245,0.1351354,0.0017632558,1.0
800000,1.2472183,920.1545454545454,4.788218,45.32318575382233,45.32318575382233,0.13445225,0.023164887,7.605692e-05,0.12535228,0.0012750787,1.0
900000,1.2449214,933.2818181818182,4.7956963,46.09213362578993,46.09213362578993,0.10228045,0.023623899,4.5109107e-05,0.11503633,0.00076031324,1.0
1000000,1.243223,921.7570093457944,4.833961,45.513765422576064,45.513765422576064,0.11884947,0.022677107,1.4265609e-05,0.10475516,0.00024728302,1.0
