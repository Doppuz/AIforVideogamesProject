Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
100000,1.4110588,30.911176663389053,-2.7862825,-3.442854784700137,-3.442854784700137,3.1691241,0.025274467,0.0002845883,0.19486277,0.0047436524,1.0
200000,1.3807166,62.00330906684315,-2.234591,-1.8998344110227594,-1.8998344110227594,1.0683954,0.023092559,0.00025526973,0.18508992,0.0042559854,1.0
300000,1.3309386,174.28269230769232,-0.56894714,3.7821160045953897,3.7821160045953897,1.006151,0.024899155,0.00022441591,0.17480527,0.0037427838,1.0
400000,1.2943648,378.8844621513944,1.0001928,14.406973628883819,14.406973628883819,0.8055719,0.02391224,0.00019508504,0.16502835,0.0032549137,1.0
500000,1.2600265,547.3005780346821,2.3726416,23.824568744339693,23.824568744339693,0.6031676,0.023283008,0.00016571503,0.1552383,0.0027663922,1.0
600000,1.2422345,665.1666666666666,3.5306892,30.653848943037865,30.653848943037865,0.47018123,0.025273222,0.00013481187,0.14493725,0.0022523692,1.0
700000,1.2300218,740.8712121212121,4.10693,34.798109199061535,34.798109199061535,0.4011702,0.02430319,0.00010542905,0.13514298,0.001763635,1.0
800000,1.2252347,779.59375,4.3860006,37.09648769721389,37.09648769721389,0.30810732,0.0233598,7.608467e-05,0.12536153,0.0012755404,1.0
900000,1.2208263,775.1404958677685,4.548584,36.92892892695656,36.92892892695656,0.28173155,0.023588588,4.5179553e-05,0.115059815,0.00076148496,1.0
1000000,1.2177199,876.2066115702479,4.646062,42.73347482996539,42.73347482996539,0.1893408,0.022183362,1.4266329e-05,0.10475542,0.00024729496,1.0
