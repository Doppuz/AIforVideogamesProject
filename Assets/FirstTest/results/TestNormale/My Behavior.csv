Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
100000,1.4102587,34.913636363636364,-0.10734699,-3.2543181720105085,-3.2543181720105085,5.44752,0.024575757,0.00028456398,0.19485466,0.004743247,1.0
200000,1.3720787,123.85042016806723,-0.24167304,1.1925214094274184,1.1925214094274184,1.1693568,0.021746265,0.00025524053,0.18508017,0.0042554997,1.0
300000,1.3408278,546.4099378881988,1.4373906,23.536252273619176,23.536252273619176,0.53833455,0.023146132,0.0002243371,0.17477903,0.0037414734,1.0
400000,1.3224206,768.5263157894736,2.9079754,36.60261521766435,36.60261521766435,0.28702423,0.021867922,0.00019494779,0.16498259,0.0032526306,1.0
500000,1.3140143,848.6837606837607,4.0031934,41.01667029022152,41.01667029022152,0.24978587,0.022581808,0.00016555829,0.15518607,0.002763785,1.0
600000,1.3044921,881.3454545454546,4.499172,42.87773103497245,42.87773103497245,0.19305186,0.024728656,0.00013459541,0.14486511,0.0022487692,1.0
700000,1.3000292,854.3193277310925,4.700451,41.45042380365003,41.45042380365003,0.22448653,0.024365027,0.00010520054,0.13506682,0.0017598341,1.0
800000,1.2995416,853.6465517241379,4.7306695,41.51336571060378,41.51336571060378,0.2147425,0.023694629,7.579683e-05,0.12526558,0.0012707526,1.0
900000,1.2975509,890.5405405405405,4.7701926,43.53063443974332,43.53063443974332,0.1613904,0.023782376,4.4875436e-05,0.11495845,0.0007564266,1.0
1000000,1.2968274,893.5132743362832,4.7935677,43.73170024795191,43.73170024795191,0.1431931,0.024440562,1.4031366e-05,0.104677096,0.00024338676,1.0
