Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
100000,1.4095689,45.381087470449174,-5.389636,-41.84030319421835,-41.84030319421835,89.5986,0.02272502,0.0002948725,0.19829085,0.0049147126,1.0
200000,1.3867135,43.164041994750654,-9.291665,-35.68075290890712,-35.68075290890712,54.63269,0.023778284,0.0002851213,0.19504043,0.0047525167,1.0
300000,1.366134,39.95315682281059,-10.991278,-31.352505664354428,-31.352505664354428,41.02339,0.023030167,0.00027485433,0.19161811,0.004581744,1.0
400000,1.343676,37.93693693693694,-12.059707,-29.87961277462574,-29.87961277462574,36.668304,0.023335991,0.00026508674,0.18836223,0.004419276,1.0
500000,1.3182218,36.036859565057135,-12.457524,-27.588703134052874,-27.588703134052874,31.748371,0.026522864,0.00025532665,0.18510887,0.0042569325,1.0
600000,1.2989422,35.18815837268434,-12.442625,-26.199601039079333,-26.199601039079333,27.271631,0.023546025,0.00024506304,0.18168768,0.0040862146,1.0
700000,1.2788086,35.54369992716679,-12.488694,-26.240605123248834,-26.240605123248834,27.266647,0.023431858,0.00023479026,0.17826343,0.0039153444,1.0
800000,1.2602836,34.29304286718201,-12.061679,-24.917586720961545,-24.917586720961545,23.953346,0.02579375,0.00022503015,0.17501003,0.0037530002,1.0
900000,1.2433809,33.706393328700486,-11.912665,-24.72246421133171,-24.72246421133171,24.14304,0.025751421,0.00021526338,0.17175445,0.0035905465,1.0
1000000,1.2289972,33.160616438356165,-11.6567,-23.672150562271522,-23.672150562271522,21.831017,0.02380671,0.00020499714,0.16833237,0.0034197848,1.0
