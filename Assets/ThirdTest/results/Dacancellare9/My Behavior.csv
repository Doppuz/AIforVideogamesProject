Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
100000,1.4146417,54.18791946308725,-10.643277,-50.21541735461523,-50.21541735461523,103.96874,0.022616467,0.00029485696,0.19828565,0.0049144533,1.0
200000,1.3963242,44.52764976958525,-15.227907,-38.30776536527867,-38.30776536527867,90.76878,0.038524065,0.00028508817,0.19502941,0.004751966,1.0
300000,1.3954686,43.76732673267327,-22.637037,-35.30620831092949,-35.30620831092949,310.33203,0.10108815,0.00027481187,0.19160394,0.004581037,1.0
400000,1.4095119,44.13135011441648,-22.981667,-32.94219725633922,-32.94219725633922,361.8633,0.10498185,0.00026505475,0.18835159,0.004418744,1.0
500000,1.4168948,39.68669705760464,-20.59632,-30.56745486423546,-30.56745486423546,209.23189,0.0965467,0.0002553015,0.18510051,0.004256515,1.0
600000,1.416026,39.35760781122864,-17.119993,-29.52654642504439,-29.52654642504439,91.30206,0.05739475,0.00024502503,0.181675,0.0040855827,1.0
700000,1.4047518,39.28161570403926,-15.623895,-30.39901868492545,-30.39901868492545,57.24409,0.037903376,0.00023473697,0.17824566,0.0039144573,1.0
