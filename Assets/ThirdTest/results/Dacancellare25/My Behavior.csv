Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
100000,1.4239702,78.85572139303483,0.5667324,1.9263487377107391,1.9263487377107391,1.132608,0.024258204,0.00029484418,0.19828138,0.004914241,1.0
200000,1.4140073,114.65207100591716,1.3008276,3.7539010608731713,3.7539010608731713,0.752396,0.023672055,0.00028508116,0.19502705,0.0047518495,1.0
300000,1.3974121,136.08892128279882,1.6620636,4.804446498437109,4.804446498437109,0.7387865,0.02378209,0.00027480331,0.19160108,0.004580894,1.0
400000,1.3876605,221.70289855072463,2.1906183,9.085145750915371,9.085145750915371,0.60489476,0.022021823,0.00026502868,0.18834288,0.0044183102,1.0
500000,1.3736824,372.87336244541484,3.0317335,16.643669610981338,16.643669610981338,0.4712546,0.024990385,0.0002552433,0.1850811,0.0042555463,1.0
600000,1.3596103,680.6095238095238,3.808273,32.03047905649458,32.03047905649458,0.3155449,0.022318123,0.00024495687,0.18165226,0.0040844493,1.0
700000,1.352651,1210.75,4.3237944,58.53750522155315,58.53750522155315,0.21040504,0.02262553,0.0002346624,0.1782208,0.0039132168,1.0
