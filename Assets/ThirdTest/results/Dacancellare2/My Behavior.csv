Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
100000,1.4159023,38.24500831946755,2.9618073,-4.781077788443557,-4.781077788443557,11.2335615,0.022646531,0.0002948435,0.19828117,0.0049142297,1.0
200000,1.4019877,60.854109589041094,0.4924424,-4.3482259709923845,-4.3482259709923845,2.836632,0.024118517,0.0002850555,0.1950185,0.0047514224,1.0
300000,1.3798124,97.68735891647856,-0.3383998,-3.3283989649673393,-3.3283989649673393,1.1730592,0.025779407,0.00027475544,0.19158514,0.004580098,1.0
400000,1.3655343,130.9078341013825,-0.96320105,-2.2452071708742922,-2.2452071708742922,8.509438,0.07574243,0.0002649837,0.18832791,0.004417562,1.0
500000,1.3621356,168.95238095238096,0.5542631,-0.7577052751547976,-0.7577052751547976,7.658474,0.059200633,0.00025521684,0.18507227,0.004255107,1.0
600000,1.3565457,239.46506024096385,0.94067276,2.5417837666055885,2.5417837666055885,12.391736,0.046693977,0.00024492838,0.18164279,0.004083975,1.0
