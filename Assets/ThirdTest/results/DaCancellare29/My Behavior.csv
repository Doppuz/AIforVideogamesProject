Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
100000,1.4182506,29.55452865064695,2.3530724,-8.522018429090503,-8.522018429090503,38.53993,0.025743706,0.00029229678,0.19743228,0.0048718695,1.0
200000,1.4088329,33.99649368863955,-1.428535,-8.300718979181553,-8.300718979181553,14.242635,0.02543793,0.00027767202,0.19255733,0.004628611,1.0
300000,1.3931626,35.513075506445674,-3.7793865,-8.224364590161851,-8.224364590161851,6.600866,0.02340301,0.00026226178,0.18742058,0.0043722875,1.0
400000,1.3820982,38.823974512146556,-5.227075,-8.05877431132525,-8.05877431132525,3.9289372,0.023441022,0.00024761062,0.18253686,0.0041285893,1.0
500000,1.3630075,41.54055201698514,-5.7750096,-7.9229723241931564,-7.9229723241931564,3.2237487,0.02337518,0.00023296748,0.17765582,0.0038850245,1.0
600000,1.346189,43.77054333183655,-5.80517,-7.811472736456279,-7.811472736456279,3.1184,0.022017267,0.00021751966,0.17250654,0.0036280763,1.0
