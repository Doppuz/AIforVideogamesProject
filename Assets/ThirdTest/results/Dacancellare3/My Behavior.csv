Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
100000,1.4213291,39.34639965913933,-1.2774495,-7.422368934396282,-7.422368934396282,8.327486,0.023497127,0.00029485067,0.19828355,0.0049143494,1.0
200000,1.4122771,58.08103015075377,-3.4319131,-8.54296035727452,-8.54296035727452,0.740576,0.02571046,0.00028507833,0.19502613,0.004751803,1.0
300000,1.4014825,67.57015130674003,-5.797903,-8.701106485286632,-8.701106485286632,5.16456,0.06948168,0.0002748047,0.19160157,0.004580918,1.0
400000,1.4131998,54.201488265598165,-6.3746943,-7.590394943472772,-7.590394943472772,14.280809,0.09445283,0.0002650266,0.1883422,0.0044182753,1.0
500000,1.4269426,50.7734682405846,-5.535142,-7.278189977068724,-7.278189977068724,83.28766,0.062339842,0.00025521862,0.18507285,0.0042551355,1.0
600000,1.4140321,57.081414935429535,-4.831678,-7.301336301964431,-7.301336301964431,0.61375946,0.02787514,0.0002449248,0.18164161,0.0040839156,1.0
700000,1.3990787,45.0043057050592,-4.305201,-6.487714740338517,-6.487714740338517,0.38543683,0.024501601,0.00023464425,0.17821474,0.003912915,1.0
